# NeuroGenesis
This repository demonstrates how neural networks operate at a low level by constructing all core components manually using basic numerical operations. It is designed for learning, experimentation, and a deeper understanding of model training dynamics.

---

## Key Features

- Manual implementation of forward propagation
- Explicit backpropagation and gradient computation
- Gradient descentâ€“based parameter updates
- ReLU and Softmax activation functions
- NumPy used only for numerical computation (no ML frameworks)

---  

## Project Goals
- Understand neural network training from first principles
- Avoid abstraction-heavy libraries (e.g., TensorFlow, PyTorch)
- Build intuition around matrix operations and gradients